{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "306efd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e2b291",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = os.getenv(\"YOLO_FOLDER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5ef5c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.4.9 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.4.7  Python-3.12.0 torch-2.9.1+cpu CPU (11th Gen Intel Core i7-1185G7 @ 3.00GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, angle=1.0, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=datasets\\vaca\\data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=3, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=src\\yolo\\yolo26n.pt, momentum=0.937, mosaic=1.0, multi_scale=0.0, name=train2, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, rle=1.0, save=True, save_conf=False, save_crop=False, save_dir=C:\\_projects\\dev\\smart_inventory\\runs\\detect\\train2, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5, 3, True]        \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    119808  ultralytics.nn.modules.block.C3k2            [384, 128, 1, True]           \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     34304  ultralytics.nn.modules.block.C3k2            [256, 64, 1, True]            \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     95232  ultralytics.nn.modules.block.C3k2            [192, 128, 1, True]           \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    463104  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True, 0.5, True]\n",
      " 23        [16, 19, 22]  1    241566  ultralytics.nn.modules.head.Detect           [1, 1, True, [64, 128, 256]]  \n",
      "YOLO26n summary: 260 layers, 2,504,190 parameters, 2,504,190 gradients, 5.8 GFLOPs\n",
      "\n",
      "Transferred 606/708 items from pretrained weights\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.20.0 ms, read: 54.321.2 MB/s, size: 4141.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\_projects\\dev\\smart_inventory\\datasets\\vaca\\labels\\train... 797 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 797/797 33.4it/s 23.8s0.2ss\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: C:\\_projects\\dev\\smart_inventory\\datasets\\vaca\\labels\\train.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 35.27.4 MB/s, size: 2462.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\_projects\\dev\\smart_inventory\\datasets\\vaca\\labels\\val... 43 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 43/43 34.4it/s 1.2s0.2s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: C:\\_projects\\dev\\smart_inventory\\datasets\\vaca\\labels\\val.cache\n",
      "Plotting labels to C:\\_projects\\dev\\smart_inventory\\runs\\detect\\train2\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m MuSGD(lr=0.002, momentum=0.9) with parameter groups 114 weight(decay=0.0), 126 weight(decay=0.0005), 126 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mC:\\_projects\\dev\\smart_inventory\\runs\\detect\\train2\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/3         0G      3.141      3.608   0.005972       3320        640: 100% ━━━━━━━━━━━━ 50/50 81.1s/it 1:07:3634.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 41.6s/it 1:23<3:56\n",
      "                   all         43       8656    0.00643    0.00959    0.00327    0.00108\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        2/3         0G      2.738      2.441    0.00395       4901        640: 100% ━━━━━━━━━━━━ 50/50 44.1s/it 36:4442.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 38.2s/it 1:16<3:41\n",
      "                   all         43       8656      0.352      0.274       0.21     0.0671\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        3/3         0G      2.648      1.528     0.0036       2927        640: 100% ━━━━━━━━━━━━ 50/50 47.5s/it 39:3641.7s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 49.1s/it 1:38<4:38\n",
      "                   all         43       8656      0.388      0.289       0.24     0.0777\n",
      "\n",
      "3 epochs completed in 2.472 hours.\n",
      "Optimizer stripped from C:\\_projects\\dev\\smart_inventory\\runs\\detect\\train2\\weights\\last.pt, 5.4MB\n",
      "Optimizer stripped from C:\\_projects\\dev\\smart_inventory\\runs\\detect\\train2\\weights\\best.pt, 5.4MB\n",
      "\n",
      "Validating C:\\_projects\\dev\\smart_inventory\\runs\\detect\\train2\\weights\\best.pt...\n",
      "Ultralytics 8.4.7  Python-3.12.0 torch-2.9.1+cpu CPU (11th Gen Intel Core i7-1185G7 @ 3.00GHz)\n",
      "YOLO26n summary (fused): 122 layers, 2,375,031 parameters, 0 gradients, 5.2 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 2/2 9.1s/it 18.1s<35.6s\n",
      "                   all         43       8656      0.388      0.288       0.24     0.0776\n",
      "Speed: 2.0ms preprocess, 173.9ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\_projects\\dev\\smart_inventory\\runs\\detect\\train2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolo26n.yaml\")\n",
    "\n",
    "# Load a pretrained YOLO model (recommended for training)\n",
    "model = YOLO(fr\"{folder_path}\\yolo26n.pt\")\n",
    "\n",
    "# Train the model using the 'coco8.yaml' dataset for 3 epochs\n",
    "results = model.train(data=r\"datasets\\vaca\\data.yaml\", epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8b599f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.4.7  Python-3.12.0 torch-2.9.1+cpu CPU (11th Gen Intel Core i7-1185G7 @ 3.00GHz)\n",
      "YOLO26n summary (fused): 122 layers, 2,375,031 parameters, 0 gradients, 5.2 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.0 ms, read: 362.694.1 MB/s, size: 2460.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\_projects\\dev\\smart_inventory\\datasets\\vaca\\labels\\val.cache... 43 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 43/43  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 3/3 4.5s/it 13.5s.1s7s\n",
      "                   all         43       8656      0.388      0.288       0.24     0.0776\n",
      "Speed: 1.0ms preprocess, 137.7ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\_projects\\dev\\smart_inventory\\runs\\detect\\val2\u001b[0m\n",
      "\n",
      "image 1/1 c:\\_projects\\dev\\smart_inventory\\data\\ChatGPT Image Jan 28, 2026, 08_38_52 AM.png: 448x640 1 cow, 198.5ms\n",
      "Speed: 3.1ms preprocess, 198.5ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Ultralytics 8.4.7  Python-3.12.0 torch-2.9.1+cpu CPU (11th Gen Intel Core i7-1185G7 @ 3.00GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'C:\\_projects\\dev\\smart_inventory\\runs\\detect\\train2\\weights\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 300, 6) (5.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.20.1 opset 22...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\_projects\\dev\\smart_inventory\\.venv\\Lib\\site-packages\\torch\\onnx\\_internal\\torchscript_exporter\\utils.py:1447: OnnxExporterWarning: Exporting to ONNX opset version 22 is not supported. by 'torch.onnx.export()'. The highest opset version supported is 20. To use a newer opset version, consider 'torch.onnx.export(..., dynamo=True)'. \n",
      "  warnings.warn(\n",
      "c:\\_projects\\dev\\smart_inventory\\.venv\\Lib\\site-packages\\torch\\onnx\\_internal\\torchscript_exporter\\symbolic_opset9.py:5353: UserWarning: Exporting aten::index operator of advanced indexing in opset 22 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.82...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  3.8s, saved as 'C:\\_projects\\dev\\smart_inventory\\runs\\detect\\train2\\weights\\best.onnx' (9.4 MB)\n",
      "\n",
      "Export complete (4.3s)\n",
      "Results saved to \u001b[1mC:\\_projects\\dev\\smart_inventory\\runs\\detect\\train2\\weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=C:\\_projects\\dev\\smart_inventory\\runs\\detect\\train2\\weights\\best.onnx imgsz=640 \n",
      "Validate:        yolo val task=detect model=C:\\_projects\\dev\\smart_inventory\\runs\\detect\\train2\\weights\\best.onnx imgsz=640 data=datasets\\vaca\\data.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate the model's performance on the validation set\n",
    "results = model.val()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69cecd21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 c:\\_projects\\dev\\smart_inventory\\datasets\\vaca\\images\\val\\vj32_jpg.rf.26e063f5a917a9e05dedcb26ab7726d1.jpg: 384x640 141 cows, 240.0ms\n",
      "Speed: 3.1ms preprocess, 240.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Ultralytics 8.4.7  Python-3.12.0 torch-2.9.1+cpu CPU (11th Gen Intel Core i7-1185G7 @ 3.00GHz)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'C:\\_projects\\dev\\smart_inventory\\runs\\detect\\train2\\weights\\best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 300, 6) (5.1 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.20.1 opset 22...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.82...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  2.6s, saved as 'C:\\_projects\\dev\\smart_inventory\\runs\\detect\\train2\\weights\\best.onnx' (9.4 MB)\n",
      "\n",
      "Export complete (3.0s)\n",
      "Results saved to \u001b[1mC:\\_projects\\dev\\smart_inventory\\runs\\detect\\train2\\weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=C:\\_projects\\dev\\smart_inventory\\runs\\detect\\train2\\weights\\best.onnx imgsz=640 \n",
      "Validate:        yolo val task=detect model=C:\\_projects\\dev\\smart_inventory\\runs\\detect\\train2\\weights\\best.onnx imgsz=640 data=datasets\\vaca\\data.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform object detection on an image using the model\n",
    "results = model(r\"datasets\\vaca\\images\\val\\vj32_jpg.rf.26e063f5a917a9e05dedcb26ab7726d1.jpg\")\n",
    "\n",
    "# Export the model to ONNX format\n",
    "success = model.export(format=\"onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cb8cc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e53662",
   "metadata": {},
   "source": [
    "## Treinar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b720f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a YOLO model\n",
    "model = YOLO(\"yolo26n.yaml\")\n",
    "\n",
    "# Train the model\n",
    "model.train(data=\"coco8.yaml\", epochs=5)\n",
    "\n",
    "# Validate on training data\n",
    "model.val()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f4ea75",
   "metadata": {},
   "source": [
    "## Prever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970676e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolo26n.pt\")\n",
    "# accepts all formats - image/dir/Path/URL/video/PIL/ndarray. 0 for webcam\n",
    "results = model.predict(source=\"0\")\n",
    "results = model.predict(source=\"folder\", show=True)  # Display preds. Accepts all YOLO predict arguments\n",
    "\n",
    "# from PIL\n",
    "im1 = Image.open(\"bus.jpg\")\n",
    "results = model.predict(source=im1, save=True)  # save plotted images\n",
    "\n",
    "# from ndarray\n",
    "im2 = cv2.imread(\"bus.jpg\")\n",
    "results = model.predict(source=im2, save=True, save_txt=True)  # save predictions as labels\n",
    "\n",
    "# from list of PIL/ndarray\n",
    "results = model.predict(source=[im1, im2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
